# AutoMQ Project Ideas

If you are a GSoC candidate, please check out our **Contributor Guidance** to learn more about our project requirements, communication channels, and how to submit a successful proposal.

If a project is successfully selected, you will be allocated a primary mentor and supported by the rest of the team. If you are interested in learning more about a particular project idea please contact us using kaiming.wan@automq.com or on our [AutoMQ Slack channel](https://go.automq.com/slack).

## Idea 1: AutoMQ Table Topic Solution Workshop

**Objective:**

To build hands-on, visually impressive industry solution templates (e.g., FinTech, Gaming, or IoT) that demonstrate AutoMQ's "Table Topic" capabilityâ€”automatically transforming Kafka streams into Apache Iceberg tables.

**Description:**

How can AutoMQ Table Topic simplify real-time data architectures? In this project, you will create end-to-end "Showcase Demos." You'll use AI-driven development (Vibe Coding with Claude) to build data generators and pipelines, focusing on the final "Hands-on" experience and high-quality visualization. You are encouraged to explore any industry scenario you are passionate about.

**What you will do:**

- Design and implement 2-3 industry demo scenarios (e.g., Anti-fraud, IoT monitoring).
- Use AI (Claude) to quickly generate data simulators and processing logic.
- Create beautiful, interactive dashboards (using Grafana, Streamlit, or custom web apps) to visualize the data in Iceberg.
- Produce a "Quick Start" guide for each solution to help users experience the flow in minutes.

**Skills Required:**

- Experience with AI-assisted coding (Prompt engineering).
- Basic understanding of data streaming (Kafka/Iceberg).
- Language-agnostic (Python, Java, Go, or Node.js are all welcome).

**Difficulty:** Medium

**Project Size:** 175 hours (Medium)

## Idea 2: Multi-Catalog Integration for AutoMQ Table Topic

**Objective:**

To make AutoMQ Table Topics instantly discoverable by connecting them to mainstream data catalogs like Apache Polaris or Unity Catalog.

**Description:**

AutoMQ transforms Kafka data into Iceberg tables automatically. This project focuses on integrating these tables with popular catalogs. Using AI/Vibe Coding (Claude), you will bridge the metadata gap, enabling a "Zero-ETL" experience where streaming data is immediately queryable by external SQL engines.

**What you will do:**

- Integrate AutoMQ with 2+ catalogs (e.g., Polaris, Unity, or DataHub) to sync metadata.
- Use AI to accelerate API mapping and integration logic.
- Demonstrate "Stream-to-SQL" by querying data via Trino or StarRocks.
- Provide a simple docker-compose environment for the community.

**Skills Required:**

- AI-assisted coding, basic REST API knowledge, and curiosity about Data Lakes.

**Difficulty:** Medium

**Project Size:** 90~120 hours (Medium)
